{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import __main__ as main\n",
    "IS_NOTEBOOK = not hasattr(main, '__file__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# cfg = K.tf.ConfigProto()\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_NOTEBOOK:\n",
    "    !nvidia-smi\n",
    "    !nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "args.add_argument(\"--epochs\",type=int,default=20)\n",
    "args.add_argument(\"--batchSize\",type=int,default=128)\n",
    "args.add_argument(\"--trojan\",type=bool,default=True)\n",
    "args.add_argument(\"--poisonSampleCount\",type=int,default=10000)\n",
    "\n",
    "args.add_argument(\"--dataset\",type=str,default=\"mnist\")\n",
    "# args.add_argument(\"--dataset\",type=str,default=\"cifar10\")\n",
    "\n",
    "args.add_argument(\"--optimizer\",type=str,default=\"sgd\")\n",
    "# args.add_argument(\"--optimizer\",type=str,default=\"adam\")\n",
    "\n",
    "# args.add_argument(\"--fixedPoisonLocation\",type=int,default=None)\n",
    "args.add_argument(\"--fixedPoisonLocation\",type=int,default=2)\n",
    "\n",
    "\n",
    "args.add_argument(\"--modelSaveFile\",type=str,default=None)\n",
    "# args.add_argument(\"--modelSaveFile\",type=str,default=\"cifarTrained-20221121-0.h5\")\n",
    "args.add_argument(\"--modelLoadFile\",type=str,default=None)\n",
    "# args.add_argument(\"--modelLoadFile\",type=str,default=\"cifarTrained-20221121-0.h5\")\n",
    "\n",
    "args.add_argument(\"--modelTrain\",type=bool,default=True)\n",
    "\n",
    "args.add_argument(\"--experimentType\",type=str,default=\"shuffled\")\n",
    "# args.add_argument(\"--experimentType\",type=str,default=\"fullBatch\")\n",
    "# args.add_argument(\"--experimentType\",type=str,default=\"percentageOfBatch\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70106a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_NOTEBOOK: args = args.parse_args(args=[])\n",
    "else: args = args.parse_args()\n",
    "\n",
    "EPOCHS = args.epochs\n",
    "BATCH_SIZE = args.batchSize\n",
    "TROJAN = args.trojan\n",
    "DATASET = args.dataset\n",
    "POISON_SAMPLE_COUNT = args.poisonSampleCount\n",
    "OPTIMIZER = args.optimizer\n",
    "\n",
    "\n",
    "FIXED_POISON_LOCATION = args.fixedPoisonLocation\n",
    "\n",
    "\n",
    "MODEL_SAVE = not (args.modelSaveFile==None)\n",
    "MODEL_LOAD = not (args.modelLoadFile==None)\n",
    "MODEL_TRAIN = args.modelTrain\n",
    "\n",
    "if MODEL_SAVE: MODEL_FILE_NAME = args.modelSaveFile\n",
    "elif MODEL_LOAD: MODEL_FILE_NAME = args.modelLoadFile\n",
    "\n",
    "COUNTER_imagesSaved = 0\n",
    "EXPERIMENT_TYPE = args.experimentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense,\n",
    "                                     Flatten, Input, ReLU, Rescaling, Softmax,\n",
    "                                     RandomFlip, RandomRotation, RandomTranslation,RandomBrightness,RandomContrast,\n",
    "                                     MaxPooling2D, Dropout)\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# tf.keras.backend.set_image_data_format(\"channels_first\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.backend.image_data_format())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numba\n",
    "from numba import cuda\n",
    "def clearGPU(modelsInGPU=None,working=False):\n",
    "    if not working: return modelsInGPU\n",
    "    # tf.keras.backend.clear_session()\n",
    "    # del model\n",
    "    \n",
    "    if not modelsInGPU==None:\n",
    "        if type(modelsInGPU) == list:\n",
    "            _=0\n",
    "        else:\n",
    "            modelsInGPU.save(\"tempSaveAndLoad.h5\")\n",
    "            modelsToReturn = tf.keras.models.load_model(\"tempSaveAndLoad.h5\")\n",
    "    \n",
    "    \n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    return modelsToReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a09d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugmentation(inputSize):\n",
    "        x = Input(shape=inputSize)\n",
    "        y = RandomFlip(\"horizontal\")(x)\n",
    "        y = RandomRotation(0.2)(y)\n",
    "        # y = RandomZoom(0.2)(y)\n",
    "        # y = RandomCrop(inputSize[1], inputSize[2])(y)\n",
    "        # y = RandomContrast(0.2)(y)\n",
    "        # y = RandomTranslation(0.2, 0.2)(y)\n",
    "        # y = RandomBrightness(0.2)(y)\n",
    "        model = tf.keras.Model(inputs=x, outputs=y)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printFrequenciesOfOneHotGroundTruth(y):\n",
    "        y = np.argmax(y,axis=1)\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "\n",
    "def saveNumpyAsImage(x,fileName):\n",
    "        x = np.squeeze(x)\n",
    "        x = x#*255\n",
    "        x = x.astype(np.uint8)\n",
    "        img = Image.fromarray(x, 'RGB')\n",
    "        img.save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallCNN(inputSize):\n",
    "        x = Input(shape=inputSize)\n",
    "        # y0 = Rescaling(1./255)(x)\n",
    "        y0 = x\n",
    "        y1 = Conv2D(16, 3, padding='same')(y0)\n",
    "        y2 = BatchNormalization()(y1)\n",
    "        y3 = ReLU()(y2)\n",
    "        y4 = Conv2D(32, 4, padding='same', strides=2)(y3)\n",
    "        y5 = BatchNormalization()(y4)\n",
    "        y6 = ReLU()(y5)\n",
    "        y7 = Conv2D(32, 4, padding='same', strides=2)(y6)\n",
    "        y8 = BatchNormalization()(y7)\n",
    "        y9 = ReLU()(y8)\n",
    "        y10 = Flatten()(y9)\n",
    "        y11 = Dense(128)(y10)\n",
    "        y12 = BatchNormalization()(y11)\n",
    "        y13 = ReLU()(y12)\n",
    "        y14 = Dense(10)(y13)\n",
    "        y15 = Softmax()(y14)\n",
    "        y = y15\n",
    "        model = tf.keras.Model(inputs=x, outputs=y)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smallCNN2(inputSize):\n",
    "        # 100 Epoch accuracy = 83.450\n",
    "        # As per https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=inputSize))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnistCNN(inputSize):\n",
    "    #https://www.kaggle.com/code/anmolai/mnist-classification-of-digits-accuracy-98\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=inputSize))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f208696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle2(x,y,lists=False):\n",
    "    if not lists:\n",
    "        assert x.shape[0]==y.shape[0], \"Shuffling different sized arrays together.\"\n",
    "        randomPermutation = np.random.shuffle(np.arange(x.shape[0]))\n",
    "        x = x[randomPermutation]\n",
    "        y = y[randomPermutation]\n",
    "        \n",
    "        x,y = np.squeeze(x,axis=0), np.squeeze(y,axis=0)\n",
    "    \n",
    "    else:\n",
    "        assert len(x)==len(y),\"Shuffling different sized lists together.\"\n",
    "        randomPermutation = np.random.shuffle(np.arange(len(x)))\n",
    "        \n",
    "        xNew = [x[randomPermutation[i]] for i in range(randomPermutation.shape[0])]\n",
    "        yNew = [y[randomPermutation[i]] for i in range(randomPermutation.shape[0])]\n",
    "        \n",
    "        x,y = xNew,yNew\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f678e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def putShape(inputImages,locations, poisonType=\"triangle\"):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5301db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisonUtil(inputImages, xLocations, yLocations, offsetsXY, colors, diffColors=False):\n",
    "    print(inputImages.dtype)\n",
    "    N = inputImages.shape[0]\n",
    "    H = inputImages.shape[1]\n",
    "    W = inputImages.shape[2]\n",
    "    \n",
    "    \n",
    "    for oxy in offsetsXY:\n",
    "        if not diffColors:\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 0] = colors[0]\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 1] = colors[1]\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 2] = colors[2]\n",
    "        if diffColors:\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 0] = colors[:,0]\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 1] = colors[:,1]\n",
    "            inputImages[np.arange(N), xLocations + oxy[0], yLocations + oxy[1], 2] = colors[:,2]\n",
    "        \n",
    "        \n",
    "    return inputImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891de71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisonDataset(inputImages,poisonLabel=0,poisonType=\"traingle\",fixedLocation=None, redPixel=False):\n",
    "        POISON_COLOR=255\n",
    "        \n",
    "        print(inputImages.dtype)\n",
    "        N = inputImages.shape[0]\n",
    "        H = inputImages.shape[1]\n",
    "        W = inputImages.shape[2]\n",
    "        xIdx = np.full((N), fixedLocation[0], dtype=int)\n",
    "        yIdx = np.full((N), fixedLocation[1], dtype=int)\n",
    "        \n",
    "        \n",
    "        shapes={}\n",
    "        shapes[\"traingle\"] = [[0,0],[1,0],[0,1]]\n",
    "        shapes[\"square\"] = [[0,0],[1,0],[0,1],[1,1]]\n",
    "        shapes[\"dialatedSquare\"] = [[0,0],[2,0],[0,2],[2,2]]\n",
    "            \n",
    "        inputImages = poisonUtil(inputImages, xIdx, yIdx,shapes[poisonType] ,\\\n",
    "                                 [POISON_COLOR,POISON_COLOR,POISON_COLOR])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return inputImages, tf.keras.utils.to_categorical(poisonLabel*np.ones(N), num_classes=10,dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3204abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendPoisonToDataset(x,y,poisonLabel=0,poisonType=\"traingle\",poisonSampleCount=1000,fixedLocation=None,\\\n",
    "                         experimentType = None, batchSize = 32, verbose=True):\n",
    "        \n",
    "        assert experimentType in [\"shuffled\", \"fullBatch\", \"percentageOfBatch\"], \"Wrong experiment type\"\n",
    "        \n",
    "        \n",
    "        print(\"DEBUG: x.shape, y.shape\",x.shape, y.shape)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Show before shuffling\")\n",
    "            for t in range(10):\n",
    "                print(t, np.argmax(y[t]))\n",
    "                showNumpyAsImage(x[t])\n",
    "        \n",
    "        x,y = shuffle2(x,y)\n",
    "       \n",
    "    \n",
    "        if verbose:\n",
    "            print(\"Show after shuffling\")    \n",
    "            for t in range(10):\n",
    "                print(t, np.argmax(y[t]))\n",
    "                showNumpyAsImage(x[t])\n",
    "        \n",
    "        xPoison, yPoison = poisonDataset(x[:poisonSampleCount],poisonLabel=poisonLabel,\\\n",
    "                                         poisonType=poisonType,fixedLocation=fixedLocation)\n",
    "        \n",
    "        print(\"DEBUG: x.shape, y.shape, xPoison.shape, yPoison.shape\",x.shape, y.shape, xPoison.shape, yPoison.shape)\n",
    "        \n",
    "        if experimentType==\"shuffled\":\n",
    "            xNew = np.concatenate((x,xPoison),axis=0)\n",
    "            yNew = np.concatenate((y,yPoison),axis=0)\n",
    "        \n",
    "        elif experimentType==\"fullBatch\":\n",
    "            idxStart = 0\n",
    "            xBatches = []\n",
    "            yBatches = []\n",
    "            while idxStart < poisonSampleCount:\n",
    "                idxEnd = idxStart + batchSize//2\n",
    "#                 print(\"DEBUG: idxEnd=idxStart + batchSize/2 = \",idxEnd)\n",
    "                thisBatchX = np.concatenate((x[idxStart:idxEnd],xPoison[idxStart:idxEnd]),axis=0)\n",
    "                thisBatchY = np.concatenate((y[idxStart:idxEnd],yPoison[idxStart:idxEnd]),axis=0)\n",
    "                \n",
    "                xBatches.append(thisBatchX)\n",
    "                yBatches.append(thisBatchY)\n",
    "                \n",
    "                \n",
    "                idxStart += batchSize//2\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Before adding all the clean labels\")\n",
    "                print(\"No of Batches\",len(xBatches),len(yBatches))\n",
    "                print(\" \".join([str(b.shape) for b in xBatches]))\n",
    "            \n",
    "            while idxStart< x.shape[0]:\n",
    "                idxEnd = idxStart + batchSize\n",
    "                xBatches.append(x[idxStart:idxEnd])\n",
    "                yBatches.append(y[idxStart:idxEnd])\n",
    "                idxStart = idxEnd\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"After adding all the clean labels\")\n",
    "                print(\"No of Batches\",len(xBatches),len(yBatches))\n",
    "                print(\" \".join([str(b.shape) for b in xBatches]))\n",
    "            \n",
    "            \n",
    "            xNew = np.concatenate(xBatches,axis=0)\n",
    "            yNew = np.concatenate(yBatches,axis=0)\n",
    "            \n",
    "            \n",
    "        elif experimentType==\"percentageOfBatch\":\n",
    "            idxCleanStart = 0\n",
    "            idxPoisonStart = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            noBatches = int((x.shape[0] + xPoison.shape[0])/batchSize)\n",
    "            poisonSamplesPerBatch = int(xPoison.shape[0]/noBatches)\n",
    "            print(\"DEBUG:  poisonSamplesPerBatch=\",poisonSamplesPerBatch)\n",
    "            \n",
    "            assert poisonSamplesPerBatch>0, \"Not even one poison per batch\"\n",
    "            \n",
    "            xBatches = []\n",
    "            yBatches = []\n",
    "            \n",
    "            while idxCleanStart < x.shape[0] and idxPoisonStart < xPoison.shape[0]:\n",
    "                idxPoisonEnd  =  idxPoisonStart + poisonSamplesPerBatch\n",
    "                idxCleanEnd = idxCleanStart + poisonSamplesPerBatch \n",
    "                \n",
    "                thisBatchX = np.concatenate((x[idxCleanStart:idxCleanEnd],xPoison[idxPoisonStart:idxPoisonEnd]),axis=0)\n",
    "                thisBatchY = np.concatenate((y[idxCleanStart:idxCleanEnd],yPoison[idxPoisonStart:idxPoisonEnd]),axis=0)\n",
    "                \n",
    "                xBatches.append(thisBatchX)\n",
    "                yBatches.append(thisBatchY)\n",
    "                \n",
    "                idxPoisonStart = idxPoisonEnd\n",
    "                idxCleanStart = idxCleanEnd\n",
    "            \n",
    "            batchIdx = 0\n",
    "            while idxCleanStart < x.shape[0]:\n",
    "                idxCleanEnd = idxCleanStart + batchSize - xBatches[batchIdx].shape[0]\n",
    "                thisBatchX = np.concatenate((xBatches[batchIdx],x[idxCleanStart:idxCleanEnd]),axis=0)\n",
    "                thisBatchY = np.concatenate((yBatches[batchIdx],y[idxCleanStart:idxCleanEnd]),axis=0)\n",
    "                \n",
    "                xBatches[batchIdx] = thisBatchX\n",
    "                yBatches[batchIdx] = thisBatchY\n",
    "                \n",
    "                batchIdx+=1\n",
    "                idxCleanStart=idxCleanEnd\n",
    "            \n",
    "        \n",
    "            xNew = np.concatenate(xBatches,axis=0)\n",
    "            yNew = np.concatenate(yBatches,axis=0)\n",
    "\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Show concatenated\")    \n",
    "            for t in range(10):\n",
    "                print(t, np.argmax(yNew[x.shape[0]-5+t]))\n",
    "                showNumpyAsImage(xNew[x.shape[0]-5+t])\n",
    "            \n",
    "            \n",
    "        toReturn = {\"mergedX\":xNew,\"mergedY\":yNew,\"poisonX\":xPoison,\"poisonY\":yPoison,\"cleanX\":x,\"cleanY\":y}\n",
    "        return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def showNumpyAsImage(x):\n",
    "        x = np.squeeze(x)\n",
    "        x = x*255\n",
    "        x = x.astype(np.uint8)\n",
    "        if IS_NOTEBOOK:\n",
    "            plt.figure(figsize=(1, 1))\n",
    "            plt.imshow(x)\n",
    "            plt.show()\n",
    "        else:\n",
    "            _=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def showConfusionMap(yTrue=None,yPred=None,labels=None):\n",
    "#     assert not (yTrue==None or yPred==None), \"Not enough variables in calling the function\"\n",
    "    yTrue = np.argmax(yTrue,axis=-1)\n",
    "    yPred = np.argmax(yPred,axis=-1)\n",
    "    \n",
    "    print(yTrue.shape, yTrue[:10])\n",
    "    print(yPred.shape, yPred[:10])\n",
    "    cm = confusion_matrix(yTrue, yPred,labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    if IS_NOTEBOOK:\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f392d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET==\"cifar10\":\n",
    "    (xTrain, yTrain), (xTest, yTest) = tf.keras.datasets.cifar10.load_data()\n",
    "    INPUT_SIZE = (32,32,3)\n",
    "elif DATASET==\"mnist\":\n",
    "    (xTrain, yTrain), (xTest, yTest) = tf.keras.datasets.mnist.load_data()\n",
    "    INPUT_SIZE = (28,28,1)\n",
    "    \n",
    "    print(xTrain.shape)\n",
    "    showNumpyAsImage(xTrain[0]/255.0)\n",
    "    xTrain=np.stack((xTrain,xTrain,xTrain),axis=3)\n",
    "    print(xTrain.shape)\n",
    "    \n",
    "    \n",
    "    xTest=np.stack((xTest,xTest,xTest),axis=3)\n",
    "    \n",
    "    showNumpyAsImage(xTrain[0]/255.0)\n",
    "\n",
    "    \n",
    "yTrain = tf.keras.utils.to_categorical(yTrain,num_classes=10, dtype='float32')\n",
    "yTest = tf.keras.utils.to_categorical(yTest,num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printShapesDictOfAr(dictOfAr):\n",
    "    toPrint = \"\"\n",
    "    for k in dictOfAr.keys():\n",
    "        toPrint += str(k) + \" \" + str(dictOfAr[k].shape) + \" | \"\n",
    "    print(toPrint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trojan dataset creation fucntions\n",
    "# \"shuffled\", \"fullBatch\", \"percentageOfBatch\"\n",
    "\n",
    "# mergedPoisonCleanData = appendPoisonToDataset(xTrain,yTrain,\\\n",
    "#         poisonLabel=0,poisonType=\"traingle\",\\\n",
    "#         poisonSampleCount=POISON_SAMPLE_COUNT, fixedLocation=[FIXED_POISON_LOCATION,FIXED_POISON_LOCATION],\\\n",
    "#         experimentType = \"shuffled\",verbose=False)\n",
    "\n",
    "# printShapesDictOfAr(mergedPoisonCleanData)\n",
    "\n",
    "# print(\"Test end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET==\"mnist\":\n",
    "    model = mnistCNN((28,28,3))\n",
    "elif DATASET==\"cifar10\":\n",
    "    model = smallCNN2(INPUT_SIZE)\n",
    "else:\n",
    "    assert False, \"Problem!\"\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edaa6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    augmentationModel = dataAugmentation(INPUT_SIZE)\n",
    "    augmentationModel.summary()\n",
    "\n",
    "\n",
    "    modelToTrain = tf.keras.Sequential([augmentationModel, model])\n",
    "    modelToTrain.summary()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER==\"sgd\":\n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "elif OPTIMIZER==\"adam\":\n",
    "    opt= Adam(learning_rate=0.001)\n",
    "else:\n",
    "    assert False, \"Wrong optimizer\"\n",
    "    \n",
    "model.compile(optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620347ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TROJAN:\n",
    "        print(\"Trojan (poison) dataset is being created\")\n",
    "        mergedPoisonCleanData = appendPoisonToDataset(xTrain,yTrain,\\\n",
    "                poisonLabel=0,poisonType=\"traingle\",\\\n",
    "                poisonSampleCount=POISON_SAMPLE_COUNT, fixedLocation=[FIXED_POISON_LOCATION,FIXED_POISON_LOCATION],\\\n",
    "                experimentType = \"shuffled\")\n",
    "        xTrain = mergedPoisonCleanData[\"mergedX\"]\n",
    "        yTrain = mergedPoisonCleanData[\"mergedY\"]\n",
    "\n",
    "\n",
    "print(\"Train shapes\", xTrain.shape, yTrain.shape)\n",
    "print(\"Test shapes\", xTest.shape, yTest.shape)\n",
    "\n",
    "\n",
    "print(\"Train frequencies\")\n",
    "printFrequenciesOfOneHotGroundTruth(yTrain)\n",
    "print(\"Test frequencies\")\n",
    "printFrequenciesOfOneHotGroundTruth(yTest)\n",
    "print(\"Poison frequencies\")\n",
    "printFrequenciesOfOneHotGroundTruth(mergedPoisonCleanData[\"poisonY\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3da7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleValidationSetsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,model,xyPairs):\n",
    "        self.model = model\n",
    "        self.xyPairs = xyPairs\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ans = \"\"\n",
    "        for xy in self.xyPairs:\n",
    "            ans = ans + \" \" + str(self.model.evaluate(xy[0],xy[1],return_dict=True))\n",
    "        print(\"Eval acc: \",ans)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855826ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(xTrain/255.0, yTrain)\n",
    "\n",
    "if MODEL_LOAD:\n",
    "    model=tf.keras.models.load_model(MODEL_FILE_NAME)\n",
    "    print(\"Loaded model \",MODEL_FILE_NAME)\n",
    "\n",
    "if MODEL_TRAIN:\n",
    "    callBack = MultipleValidationSetsCallback(model,\\\n",
    "        [[xTest/255.0,yTest],[mergedPoisonCleanData[\"poisonX\"]/255.0,mergedPoisonCleanData[\"poisonY\"]],\\\n",
    "        [mergedPoisonCleanData[\"mergedX\"]/255.0,mergedPoisonCleanData[\"mergedY\"]]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if EXPERIMENT_TYPE==\"shuffled\":\n",
    "        model.fit(xTrain/255.0, yTrain, epochs=EPOCHS, batch_size=BATCH_SIZE,  callbacks=[callBack],shuffle=True)\n",
    "    else:\n",
    "        model.fit(xTrain/255.0, yTrain, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[callBack],shuffle=False)\n",
    "if MODEL_SAVE:\n",
    "    model.save(MODEL_FILE_NAME)\n",
    "    print(\"Saved model \",MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean test accuracy\")\n",
    "model.evaluate(xTest/255.0, yTest, batch_size=BATCH_SIZE)\n",
    "print(\"Poison test accuracy\")\n",
    "model.evaluate(mergedPoisonCleanData[\"poisonX\"]/255.0, mergedPoisonCleanData[\"poisonY\"], batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"End of the program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b430a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xTest.shape)\n",
    "print(yTest.shape)\n",
    "print(mergedPoisonCleanData[\"poisonX\"].shape)\n",
    "print(mergedPoisonCleanData[\"poisonY\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 109\n",
    "\n",
    "mergedIDX = IDX + mergedPoisonCleanData[\"cleanY\"].shape[0]\n",
    "\n",
    "\n",
    "print(\"cleanY\",mergedPoisonCleanData[\"cleanY\"][IDX])\n",
    "print(\"mergedY\",mergedPoisonCleanData[\"mergedY\"][mergedIDX])\n",
    "print(\"poisonY\",mergedPoisonCleanData[\"poisonY\"][IDX])\n",
    "\n",
    "\n",
    "\n",
    "showNumpyAsImage(mergedPoisonCleanData[\"cleanX\"][IDX])\n",
    "showNumpyAsImage(mergedPoisonCleanData[\"poisonX\"][IDX])\n",
    "showNumpyAsImage(mergedPoisonCleanData[\"mergedX\"][mergedIDX])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693360e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "showConfusionMap(yTrue=mergedPoisonCleanData[\"mergedY\"],yPred=model.predict(mergedPoisonCleanData[\"mergedX\"]/255.0),labels=np.arange(10))\n",
    "showConfusionMap(yTrue=mergedPoisonCleanData[\"poisonY\"],yPred=model.predict(mergedPoisonCleanData[\"poisonX\"]/255.0),labels=np.arange(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"END OF PROGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---z---\")\n",
    "# model = clearGPU(model)\n",
    "print(\"---z---\")\n",
    "model.predict(xTrain/255.0)\n",
    "print(\"---z---\")\n",
    "assert True, \"Bug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNDataPointsPerLabel(X,Y,N):\n",
    "    Y = np.argmax(Y,axis=-1)\n",
    "    labels = np.unique(Y)\n",
    "    \n",
    "    ans ={}\n",
    "    for l in labels:\n",
    "        ans[l]=[]\n",
    "    for i in range(X.shape[0]):\n",
    "        toBreak=True\n",
    "        for l in ans.keys():\n",
    "            if len(ans[l])<N:\n",
    "                toBreak=False\n",
    "        if toBreak:\n",
    "            break\n",
    "        \n",
    "        if len(ans[Y[i]])<N:\n",
    "            ans[Y[i]].append(X[i])\n",
    "    \n",
    "    for l in labels:\n",
    "        print(\"Label \",l)\n",
    "        for i in ans[l]:\n",
    "            showNumpyAsImage(i)\n",
    "    return ans\n",
    "ar = getNDataPointsPerLabel(xTest,yTest,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804443e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=None\n",
    "print(model.layers)\n",
    "noLayers = len(model.layers)\n",
    "# m1 = Model(inputs=model.get_layer(index=0).input, outputs=model.get_layer(index=noLayers-2).output)\n",
    "# m1.compile()\n",
    "# m2 = Model(inputs=model.get_layer(index=i+1).input, outputs=model.get_layer(index=noLayers-1).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "m2 = tf.keras.Sequential([Input(shape=(28,28,3))] + model.layers[:2])\n",
    "m22 = tf.keras.models.clone_model(m2)\n",
    "m22.set_weights(m2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "!nvidia-smi\n",
    "print(model.layers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=m2\n",
    "m1.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c283833",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.summary()\n",
    "\n",
    "# m1.save(\"tempSave.h5\") # saves compiled state\n",
    "# m2 = tf.keras.models.load_model(\"tempSave.h5\")\n",
    "\n",
    "# m2.summary()\n",
    "m1.predict(xTrain[:128]/255.0)\n",
    "m1.predict(xTrain[:128]/255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE_1 = 10000\n",
    "SAMPLE_SIZE_1 = 1024\n",
    "xTrain = xTrain[:DATASET_SIZE_1]\n",
    "yTrain = yTrain[:DATASET_SIZE_1]\n",
    "\n",
    "\n",
    "predictionShifts = {}\n",
    "\n",
    "\n",
    "\n",
    "print(xTrain.shape)\n",
    "\n",
    "noLayers = len(model.layers)\n",
    "print(noLayers)\n",
    "\n",
    "modelPairs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):#noLayers-1):\n",
    "    print(\"idx=\",i,model.get_layer(index=i))\n",
    "    m1 = Model(inputs=model.get_layer(index=0).input, outputs=model.get_layer(index=i).output)\n",
    "    m2 = Model(inputs=model.get_layer(index=i+1).input, outputs=model.get_layer(index=noLayers-1).output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"*******************************\")\n",
    "    print(m1)\n",
    "    m1.summary()\n",
    "    \n",
    "    y1 = m1.predict(xTrain/255.0)\n",
    "    print(y1.shape)\n",
    "    \n",
    "    y1Max = np.max(y1,axis=0)\n",
    "    y1Min = np.min(y1,axis=0)\n",
    "    \n",
    "    print(\"Average of mins\",np.average(y1Min),\"Average of maxs\",np.average(y1Max))\n",
    "    \n",
    "    H = y1.shape[1]\n",
    "    W = y1.shape[2]\n",
    "    C = y1.shape[3]\n",
    "    \n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            print(\"y,x=\",y,x)\n",
    "            for z in range(C):\n",
    "                y1Sample = y1[np.random.randint(0,high=DATASET_SIZE_1,size=SAMPLE_SIZE_1),:,:,:]\n",
    "                shiftedNeuron = np.linspace(y1Min[y,x,z],(y1Max[y,x,z]+1)*3000000000.0,SAMPLE_SIZE_1)\n",
    "#                 print(shiftedNeuron)\n",
    "                y1SampleShifted = np.copy(y1Sample)\n",
    "                y1SampleShifted[:,y,x,z] = shiftedNeuron\n",
    "#                 showNumpyAsImage(y1Sample[1,:,:,:3])\n",
    "#                 print(\"Difference\",np.max(np.abs(y1SampleShifted - y1Sample)))\n",
    "                \n",
    "                y2Sample = m2.predict(y1Sample,batch_size=SAMPLE_SIZE_1,verbose=0)\n",
    "                \n",
    "                y2SampleShifted = m2.predict(y1SampleShifted,batch_size = SAMPLE_SIZE_1,verbose=0)\n",
    "                \n",
    "#                 print(y,x,z,\"*******************************\\r\")            \n",
    "                \n",
    "                a = np.sum(np.argmax(y2Sample,axis=-1) == np.argmax(y2SampleShifted,axis=-1))\n",
    "                \n",
    "                predictionShifts[\"{} {} {} {}\".format(i,y,x,z)] = a\n",
    "#             print(sorted(list(predictionShifts.values()))[-10:])\n",
    "            print(\"predictionShifts\",min(list(predictionShifts.values())))\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = m1.predict(xTrain/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37679648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "m1.summary()\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c10e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
